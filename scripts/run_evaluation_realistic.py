# -*- coding: utf-8 -*-
"""
æ¨¡æ‹ŸçœŸå®åœºæ™¯çš„è¯„æµ‹è„šæœ¬ / Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¹
Simulates real-world scenario: initialize database then process new mentions

ä½¿ç”¨åœºæ™¯ï¼š
1. å‰50%çš„ORCID mentionsç”¨äºåˆå§‹åŒ–æ•°æ®åº“
2. å50%çš„mentionsç”¨äºè¯„æµ‹æ¶ˆæ­§æ•ˆæœ
"""

import json
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any, Set, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from models.author import Author
from models.database import AuthorDatabase
from disambiguation_engine.author_merger import AuthorMerger
from disambiguation_engine.decision_types import Decision


def setup_logging(debug: bool = False) -> logging.Logger:
    logger = logging.getLogger('eval_v2')
    level = logging.DEBUG if debug else logging.INFO
    logger.setLevel(level)
    
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setLevel(level)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    return logger


def load_crossref_data(file_path: str, limit: int = None) -> List[Dict[str, Any]]:
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    authors = data.get('authors', [])
    if limit:
        authors = authors[:limit]
    
    return authors


def main():
    parser = argparse.ArgumentParser(
        description='çœŸå®åœºæ™¯è¯„æµ‹ / ĞÑ†ĞµĞ½ĞºĞ° Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑƒÑĞ»Ğ¾Ğ²Ğ¸ÑÑ…'
    )
    parser.add_argument(
        '--data-file',
        type=str,
        default=r'C:\istina\materia ææ–™\æµ‹è¯•è¡¨å•\crossref.json',
        help='Crossrefæ•°æ®æ–‡ä»¶'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=100000,
        help='æ€»ä½œè€…è®°å½•é™åˆ¶'
    )
    parser.add_argument(
        '--init-ratio',
        type=float,
        default=0.5,
        help='ç”¨äºåˆå§‹åŒ–çš„æ•°æ®æ¯”ä¾‹'
    )
    parser.add_argument(
        '--min-mentions',
        type=int,
        default=2,
        help='æœ€å°ORCID mentionsæ•°'
    )
    parser.add_argument(
        '--mode',
        type=str,
        choices=['baseline', 'fs'],
        default='baseline',
        help='è¯„åˆ†æ¨¡å¼'
    )
    parser.add_argument(
        '--accept-threshold',
        type=float,
        default=0.85,
        help='MERGEé˜ˆå€¼'
    )
    parser.add_argument(
        '--reject-threshold',
        type=float,
        default=0.25,
        help='NEWé˜ˆå€¼'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='test_results/evaluation_realistic.json',
        help='è¾“å‡ºç»“æœæ–‡ä»¶'
    )
    parser.add_argument('--debug', action='store_true')
    
    args = parser.parse_args()
    logger = setup_logging(args.debug)
    
    print("=" * 80)
    print("äºŒå·é¡¹ç›® - çœŸå®åœºæ™¯è¯„æµ‹ / Realistic Scenario Evaluation")
    print("=" * 80)
    
    # åŠ è½½æ•°æ®
    logger.info(f"åŠ è½½æ•°æ®: {args.data_file}")
    authors = load_crossref_data(args.data_file, limit=args.limit)
    logger.info(f"åŠ è½½ {len(authors)} æ¡è®°å½•")
    
    # æŒ‰ORCIDåˆ†ç»„
    orcid_groups = defaultdict(list)
    for i, author in enumerate(authors):
        orcid = author.get('orcid', '')
        if orcid:
            orcid_groups[orcid].append((i, author))
    
    # è¿‡æ»¤ï¼šåªå¤„ç†æœ‰>=min_mentionsçš„ORCID
    valid_orcids = {k: v for k, v in orcid_groups.items() if len(v) >= args.min_mentions}
    
    print(f"\nã€æ•°æ®ç»Ÿè®¡ã€‘")
    print(f"  æ€»è®°å½•æ•°: {len(authors)}")
    print(f"  æœ‰ORCIDçš„å”¯ä¸€å€¼: {len(orcid_groups)}")
    print(f"  æœ‰æ•ˆORCID (>={args.min_mentions} mentions): {len(valid_orcids)}")
    
    # åˆ’åˆ†æ•°æ®ï¼šæ¯ä¸ªORCIDçš„å‰åŠéƒ¨åˆ†ç”¨äºåˆå§‹åŒ–ï¼ŒååŠéƒ¨åˆ†ç”¨äºè¯„æµ‹
    init_mentions = []
    eval_mentions = []
    
    for orcid, mention_list in valid_orcids.items():
        split_idx = max(1, int(len(mention_list) * args.init_ratio))
        init_mentions.extend(mention_list[:split_idx])
        eval_mentions.extend(mention_list[split_idx:])
    
    print(f"  åˆå§‹åŒ–mentions: {len(init_mentions)}")
    print(f"  è¯„æµ‹mentions: {len(eval_mentions)}")
    
    # åˆå§‹åŒ–æ•°æ®åº“
    logger.info("\nåˆå§‹åŒ–ä½œè€…æ•°æ®åº“...")
    database = AuthorDatabase()
    
    orcid_to_author_id = {}  # ORCID -> database author_id æ˜ å°„
    
    for idx, author_data in init_mentions:
        orcid = author_data.get('orcid', '')
        
        if orcid not in orcid_to_author_id:
            # é¦–æ¬¡è§åˆ°æ­¤ORCIDï¼Œåˆ›å»ºæ–°ä½œè€…
            new_author = database.add_author({
                'name': author_data.get('original_name', ''),
                'orcid': orcid,
                'affiliation': [author_data.get('affiliation', '')] if author_data.get('affiliation') else [],
                'journals': [author_data.get('journal', '')] if author_data.get('journal') else [],
            })
            orcid_to_author_id[orcid] = new_author.author_id
        else:
            # å·²æœ‰æ­¤ORCIDçš„ä½œè€…ï¼Œæ›´æ–°å…¶ä¿¡æ¯
            existing_id = orcid_to_author_id[orcid]
            existing_author = database.find_by_id(existing_id)
            if existing_author:
                if author_data.get('journal'):
                    existing_author.journals.add(author_data.get('journal'))
                if author_data.get('affiliation'):
                    existing_author.affiliations.add(author_data.get('affiliation'))
    
    print(f"  æ•°æ®åº“ä½œè€…æ•°: {database.get_author_count()}")
    
    # è¿è¡Œæ¶ˆæ­§è¯„æµ‹
    logger.info("\nè¿è¡Œæ¶ˆæ­§è¯„æµ‹...")
    
    merger = AuthorMerger(
        database=database,
        accept_threshold=args.accept_threshold,
        reject_threshold=args.reject_threshold,
        mode=args.mode
    )
    
    stats = {
        'total': len(eval_mentions),
        'merge': 0,
        'new': 0,
        'unknown': 0,
        'correct_merge': 0,  # MERGEä¸”æ­£ç¡®
        'wrong_merge': 0,    # MERGEä½†é”™è¯¯
    }
    
    start_time = datetime.now()
    
    for idx, author_data in eval_mentions:
        orcid = author_data.get('orcid', '')
        gold_author_id = orcid_to_author_id.get(orcid)
        
        mention = {
            'name': author_data.get('original_name', ''),
            'surname': author_data.get('lastname', ''),
            'firstname': author_data.get('firstname', ''),
            'orcid': author_data.get('orcid', ''),
            'affiliation': [author_data.get('affiliation', '')] if author_data.get('affiliation') else [],
            'journals': [author_data.get('journal', '')] if author_data.get('journal') else [],
        }
        
        result = merger.make_decision(mention)
        
        if result.decision == Decision.MERGE:
            stats['merge'] += 1
            if result.best_author_id == gold_author_id:
                stats['correct_merge'] += 1
            else:
                stats['wrong_merge'] += 1
        elif result.decision == Decision.NEW:
            stats['new'] += 1
        else:
            stats['unknown'] += 1
    
    elapsed = (datetime.now() - start_time).total_seconds()
    
    # è®¡ç®—æŒ‡æ ‡
    precision = stats['correct_merge'] / stats['merge'] if stats['merge'] > 0 else 0.0
    recall = stats['correct_merge'] / stats['total'] if stats['total'] > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    print("\n" + "=" * 80)
    print("ã€è¯„æµ‹ç»“æœ / Evaluation Resultsã€‘")
    print("=" * 80)
    
    print(f"\nğŸ“Š å†³ç­–ç»Ÿè®¡:")
    print(f"  è¯„æµ‹mentions: {stats['total']}")
    print(f"  MERGE: {stats['merge']} ({100*stats['merge']/stats['total']:.1f}%)")
    print(f"  NEW: {stats['new']} ({100*stats['new']/stats['total']:.1f}%)")
    print(f"  UNKNOWN: {stats['unknown']} ({100*stats['unknown']/stats['total']:.1f}%)")
    
    print(f"\nğŸ“Š MERGEè´¨é‡:")
    print(f"  æ­£ç¡®MERGE: {stats['correct_merge']}")
    print(f"  é”™è¯¯MERGE: {stats['wrong_merge']}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1: {f1:.4f}")
    
    print(f"\nâ±ï¸ æ€§èƒ½:")
    print(f"  å¤„ç†æ—¶é—´: {elapsed:.2f}s")
    print(f"  é€Ÿåº¦: {stats['total']/elapsed:.1f} mentions/s")
    
    print("=" * 80)
    
    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    results = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'data_file': args.data_file,
            'limit': args.limit,
            'init_ratio': args.init_ratio,
            'mode': args.mode,
            'accept_threshold': args.accept_threshold,
            'reject_threshold': args.reject_threshold,
            'elapsed_seconds': elapsed,
        },
        'data_stats': {
            'total_records': len(authors),
            'valid_orcids': len(valid_orcids),
            'init_mentions': len(init_mentions),
            'eval_mentions': len(eval_mentions),
            'database_authors': database.get_author_count(),
        },
        'decision_stats': stats,
        'metrics': {
            'precision': precision,
            'recall': recall,
            'f1': f1,
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜: {output_path}")


if __name__ == '__main__':
    main()
