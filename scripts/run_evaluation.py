# -*- coding: utf-8 -*-
"""
å®Œæ•´è¯„æµ‹è„šæœ¬ / ĞŸĞ¾Ğ»Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ / Full Evaluation Script

æµ‹è¯•äºŒå·é¡¹ç›®çš„å®Œæ•´æ¶ˆæ­§æµç¨‹ï¼Œä½¿ç”¨ORCIDä½œä¸ºé‡‘æ ‡å‡†è¿›è¡Œè¯„ä¼°
Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğ³Ğ¾ Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ° Ğ´Ğ¸Ğ·Ğ°Ğ¼Ğ±Ğ¸Ğ³ÑƒĞ°Ñ†Ğ¸Ğ¸ Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸ĞµĞ¼ ORCID Ğ² ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğµ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°
"""

import json
import sys
import argparse
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Any, Set, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from models.author import Author
from models.database import AuthorDatabase
from disambiguation_engine.author_merger import AuthorMerger
from disambiguation_engine.similarity_scorer import SimilarityScorer
from disambiguation_engine.decision_types import Decision, DecisionResult
from disambiguation_engine.decision_trace import DecisionTraceLogger


def setup_logging(debug: bool = False) -> logging.Logger:
    """é…ç½®æ—¥å¿— / ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""
    logger = logging.getLogger('evaluation')
    level = logging.DEBUG if debug else logging.INFO
    logger.setLevel(level)
    
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setLevel(level)
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    
    return logger


def load_crossref_data(file_path: str, limit: int = None) -> List[Dict[str, Any]]:
    """åŠ è½½Crossrefæ•°æ® / Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Crossref"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    authors = data.get('authors', [])
    if limit:
        authors = authors[:limit]
    
    return authors


def build_gold_set(authors: List[Dict[str, Any]], min_mentions: int = 2) -> Dict[str, Any]:
    """
    ä»ä½œè€…æ•°æ®æ„å»ºORCIDé‡‘æ ‡å‡† / ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ° Ğ¸Ğ· Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ²
    
    Returns:
        gold_set: {
            'orcid_to_mention_ids': {orcid: [mention_ids]},
            'mention_to_orcid': {mention_id: orcid},
            'mentions': {mention_id: author_data}
        }
    """
    orcid_clusters = defaultdict(list)
    mentions = {}
    
    for i, author in enumerate(authors):
        orcid = author.get('orcid', '')
        mention_id = i
        
        # å­˜å‚¨mention
        mentions[mention_id] = author
        
        # å¦‚æœæœ‰ORCIDï¼ŒåŠ å…¥cluster
        if orcid:
            orcid_clusters[orcid].append(mention_id)
    
    # è¿‡æ»¤ï¼šåªä¿ç•™æœ‰ >= min_mentions çš„ORCID
    filtered_clusters = {
        orcid: mention_ids 
        for orcid, mention_ids in orcid_clusters.items() 
        if len(mention_ids) >= min_mentions
    }
    
    # æ„å»ºåå‘æ˜ å°„
    mention_to_orcid = {}
    for orcid, mention_ids in filtered_clusters.items():
        for mid in mention_ids:
            mention_to_orcid[mid] = orcid
    
    return {
        'orcid_to_mention_ids': filtered_clusters,
        'mention_to_orcid': mention_to_orcid,
        'mentions': mentions,
        'stats': {
            'total_mentions': len(authors),
            'mentions_with_orcid': sum(1 for a in authors if a.get('orcid')),
            'unique_orcids': len(orcid_clusters),
            'filtered_orcids': len(filtered_clusters),
            'mentions_in_gold_set': len(mention_to_orcid)
        }
    }


def run_disambiguation(
    authors: List[Dict[str, Any]],
    gold_set: Dict[str, Any],
    config: Dict[str, Any],
    logger: logging.Logger
) -> Tuple[Dict[str, List[int]], Dict[str, Any]]:
    """
    è¿è¡Œæ¶ˆæ­§ç®—æ³• / Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ´Ğ¸Ğ·Ğ°Ğ¼Ğ±Ğ¸Ğ³ÑƒĞ°Ñ†Ğ¸Ğ¸
    
    Returns:
        predicted_clusters: {cluster_id: [mention_ids]}
        stats: ç»Ÿè®¡ä¿¡æ¯ / Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    """
    # åˆå§‹åŒ–ç»„ä»¶
    database = AuthorDatabase()
    merger = AuthorMerger(
        database=database,
        accept_threshold=config.get('accept_threshold', 0.90),
        reject_threshold=config.get('reject_threshold', 0.20),
        mode=config.get('mode', 'baseline')
    )
    
    # ç»Ÿè®¡
    stats = {
        'total_processed': 0,
        'merge_decisions': 0,
        'new_decisions': 0,
        'unknown_decisions': 0,
    }
    
    # é¢„æµ‹çš„clusters: author_id -> [mention_ids]
    predicted_clusters = defaultdict(list)
    mention_to_predicted = {}  # mention_id -> predicted_cluster_id
    
    # åªå¤„ç†é‡‘æ ‡å‡†ä¸­çš„mentions
    gold_mention_ids = set(gold_set['mention_to_orcid'].keys())
    
    logger.info(f"å¼€å§‹æ¶ˆæ­§å¤„ç† / ĞĞ°Ñ‡Ğ°Ğ»Ğ¾ Ğ´Ğ¸Ğ·Ğ°Ğ¼Ğ±Ğ¸Ğ³ÑƒĞ°Ñ†Ğ¸Ğ¸: {len(gold_mention_ids)} mentions")
    
    for i, author_data in enumerate(authors):
        mention_id = i
        
        # åªå¤„ç†é‡‘æ ‡å‡†ä¸­çš„mentions
        if mention_id not in gold_mention_ids:
            continue
        
        stats['total_processed'] += 1
        
        # æ„å»ºmentionå­—å…¸
        mention = {
            'name': author_data.get('original_name', ''),
            'surname': author_data.get('surname', ''),
            'firstname': author_data.get('firstname', ''),
            'orcid': author_data.get('orcid', ''),
            'affiliation': [author_data.get('affiliation', '')] if author_data.get('affiliation') else [],
            'doi': author_data.get('doi', ''),
            'journals': [author_data.get('journal', '')] if author_data.get('journal') else [],
            'coauthors': author_data.get('coauthors', []) if isinstance(author_data.get('coauthors'), list) else [],
        }
        
        # è¿è¡Œä¸‰åˆ†å†³ç­–
        result = merger.make_decision(mention)
        
        if result.decision == Decision.MERGE:
            stats['merge_decisions'] += 1
            cluster_id = result.best_author_id
            predicted_clusters[cluster_id].append(mention_id)
            mention_to_predicted[mention_id] = cluster_id
        elif result.decision == Decision.NEW:
            stats['new_decisions'] += 1
            # åˆ›å»ºæ–°author
            new_author = database.add_author({
                'canonical_name': mention['name'],
                'surnames': [mention.get('surname', '')],
                'orcid': mention.get('orcid', ''),
                'affiliations': mention.get('affiliation', []),
            })
            cluster_id = new_author.author_id
            predicted_clusters[cluster_id].append(mention_id)
            mention_to_predicted[mention_id] = cluster_id
        else:  # UNKNOWN
            stats['unknown_decisions'] += 1
            # UNKNOWN: åˆ›å»ºä¸´æ—¶clusterï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            temp_cluster_id = f"unknown_{mention_id}"
            predicted_clusters[temp_cluster_id].append(mention_id)
            mention_to_predicted[mention_id] = temp_cluster_id
        
        # è¿›åº¦æ—¥å¿—
        if stats['total_processed'] % 5000 == 0:
            logger.info(f"  å·²å¤„ç† / ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ½Ğ¾: {stats['total_processed']}")
    
    logger.info(f"æ¶ˆæ­§å®Œæˆ / Ğ”Ğ¸Ğ·Ğ°Ğ¼Ğ±Ğ¸Ğ³ÑƒĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°:")
    logger.info(f"  - MERGE: {stats['merge_decisions']}")
    logger.info(f"  - NEW: {stats['new_decisions']}")
    logger.info(f"  - UNKNOWN: {stats['unknown_decisions']}")
    logger.info(f"  - Predicted clusters: {len(predicted_clusters)}")
    
    stats['mention_to_predicted'] = mention_to_predicted
    
    return dict(predicted_clusters), stats


def evaluate_bcubed(
    gold_set: Dict[str, Any],
    mention_to_predicted: Dict[int, str]
) -> Dict[str, float]:
    """
    è®¡ç®—BÂ³ F1 / Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ BÂ³ F1
    
    BÂ³ precision: å¯¹æ¯ä¸ªmentionï¼Œè®¡ç®—å…¶predicted clusterä¸­å±äºåŒä¸€gold clusterçš„æ¯”ä¾‹
    BÂ³ recall: å¯¹æ¯ä¸ªmentionï¼Œè®¡ç®—å…¶gold clusterä¸­è¢«åˆ†åˆ°åŒä¸€predicted clusterçš„æ¯”ä¾‹
    """
    mention_to_gold = gold_set['mention_to_orcid']
    
    # åªè¯„ä¼°æœ‰gold labelçš„mentions
    mentions_to_eval = [m for m in mention_to_gold.keys() if m in mention_to_predicted]
    
    if not mentions_to_eval:
        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}
    
    # æ„å»ºåå‘ç´¢å¼•
    gold_clusters = gold_set['orcid_to_mention_ids']
    
    predicted_clusters_inv = defaultdict(set)
    for mid, cid in mention_to_predicted.items():
        predicted_clusters_inv[cid].add(mid)
    
    total_precision = 0.0
    total_recall = 0.0
    
    for mention_id in mentions_to_eval:
        gold_orcid = mention_to_gold[mention_id]
        pred_cluster_id = mention_to_predicted[mention_id]
        
        # åŒä¸€gold clusterçš„æ‰€æœ‰mentions
        gold_cluster_mentions = set(gold_clusters[gold_orcid])
        
        # åŒä¸€predicted clusterçš„æ‰€æœ‰mentions
        pred_cluster_mentions = predicted_clusters_inv[pred_cluster_id]
        
        # BÂ³ precision: |intersection with gold| / |predicted clusterä¸­æœ‰gold labelçš„|
        pred_with_gold = pred_cluster_mentions & set(mention_to_gold.keys())
        if pred_with_gold:
            intersection = pred_cluster_mentions & gold_cluster_mentions
            precision_i = len(intersection) / len(pred_with_gold)
        else:
            precision_i = 0.0
        
        # BÂ³ recall: |intersection with predicted| / |gold cluster|
        intersection = pred_cluster_mentions & gold_cluster_mentions
        recall_i = len(intersection) / len(gold_cluster_mentions)
        
        total_precision += precision_i
        total_recall += recall_i
    
    n = len(mentions_to_eval)
    precision = total_precision / n
    recall = total_recall / n
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'evaluated_mentions': n
    }


def evaluate_pairwise(
    gold_set: Dict[str, Any],
    mention_to_predicted: Dict[int, str]
) -> Dict[str, Any]:
    """
    è®¡ç®—Pairwise F1 / Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ pairwise F1
    """
    mention_to_gold = gold_set['mention_to_orcid']
    gold_clusters = gold_set['orcid_to_mention_ids']
    
    # åªè¯„ä¼°æœ‰gold labelçš„mentions
    mentions_to_eval = set(m for m in mention_to_gold.keys() if m in mention_to_predicted)
    
    if len(mentions_to_eval) < 2:
        return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'tp': 0, 'fp': 0, 'fn': 0}
    
    # ç”Ÿæˆgold pairs
    gold_pairs = set()
    for orcid, mention_ids in gold_clusters.items():
        mention_ids_in_eval = [m for m in mention_ids if m in mentions_to_eval]
        for i in range(len(mention_ids_in_eval)):
            for j in range(i + 1, len(mention_ids_in_eval)):
                pair = (min(mention_ids_in_eval[i], mention_ids_in_eval[j]),
                       max(mention_ids_in_eval[i], mention_ids_in_eval[j]))
                gold_pairs.add(pair)
    
    # ç”Ÿæˆpredicted pairs
    predicted_clusters_inv = defaultdict(list)
    for mid, cid in mention_to_predicted.items():
        if mid in mentions_to_eval:
            predicted_clusters_inv[cid].append(mid)
    
    pred_pairs = set()
    for cid, mention_ids in predicted_clusters_inv.items():
        for i in range(len(mention_ids)):
            for j in range(i + 1, len(mention_ids)):
                pair = (min(mention_ids[i], mention_ids[j]),
                       max(mention_ids[i], mention_ids[j]))
                pred_pairs.add(pair)
    
    # TP, FP, FN
    tp = len(gold_pairs & pred_pairs)
    fp = len(pred_pairs - gold_pairs)
    fn = len(gold_pairs - pred_pairs)
    
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'tp': tp,
        'fp': fp,
        'fn': fn,
        'gold_pairs': len(gold_pairs),
        'pred_pairs': len(pred_pairs)
    }


def main():
    parser = argparse.ArgumentParser(
        description='äºŒå·é¡¹ç›®å®Œæ•´è¯„æµ‹ / ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° â„–2'
    )
    parser.add_argument(
        '--data-file',
        type=str,
        default=r'C:\istina\materia ææ–™\æµ‹è¯•è¡¨å•\crossref.json',
        help='Crossrefæ•°æ®æ–‡ä»¶ / Ğ¤Ğ°Ğ¹Ğ» Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Crossref'
    )
    parser.add_argument(
        '--limit',
        type=int,
        default=50000,
        help='é™åˆ¶å¤„ç†çš„ä½œè€…æ•° / Ğ›Ğ¸Ğ¼Ğ¸Ñ‚ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ² Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸'
    )
    parser.add_argument(
        '--min-mentions',
        type=int,
        default=2,
        help='æœ€å°ORCID mentionsæ•° / ĞœĞ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ ÑƒĞ¿Ğ¾Ğ¼Ğ¸Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ½Ğ° ORCID'
    )
    parser.add_argument(
        '--mode',
        type=str,
        choices=['baseline', 'fs'],
        default='baseline',
        help='è¯„åˆ†æ¨¡å¼ / Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ñ†ĞµĞ½ĞºĞ¸: baseline (åŠ æƒ) Ğ¸Ğ»Ğ¸ fs (Fellegi-Sunter)'
    )
    parser.add_argument(
        '--accept-threshold',
        type=float,
        default=0.90,
        help='MERGEé˜ˆå€¼ / ĞŸĞ¾Ñ€Ğ¾Ğ³ MERGE'
    )
    parser.add_argument(
        '--reject-threshold',
        type=float,
        default=0.20,
        help='NEWé˜ˆå€¼ / ĞŸĞ¾Ñ€Ğ¾Ğ³ NEW'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='test_results/evaluation_results.json',
        help='è¾“å‡ºç»“æœæ–‡ä»¶ / Ğ¤Ğ°Ğ¹Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Debugæ¨¡å¼ / Ğ ĞµĞ¶Ğ¸Ğ¼ Ğ¾Ñ‚Ğ»Ğ°Ğ´ĞºĞ¸'
    )
    
    args = parser.parse_args()
    logger = setup_logging(args.debug)
    
    print("=" * 80)
    print("äºŒå·é¡¹ç›®è¯„æµ‹ / ĞÑ†ĞµĞ½ĞºĞ° Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ğ° â„–2")
    print("Incremental Author Disambiguation - Evaluation")
    print("=" * 80)
    
    # 1. åŠ è½½æ•°æ®
    logger.info(f"åŠ è½½æ•°æ® / Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {args.data_file}")
    authors = load_crossref_data(args.data_file, limit=args.limit)
    logger.info(f"åŠ è½½äº† {len(authors)} æ¡ä½œè€…è®°å½• / Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ·Ğ°Ğ¿Ğ¸ÑĞµĞ¹ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ¾Ğ²: {len(authors)}")
    
    # 2. æ„å»ºé‡‘æ ‡å‡†
    logger.info("æ„å»ºORCIDé‡‘æ ‡å‡† / ĞŸĞ¾ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ¸Ğµ Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ° ORCID...")
    gold_set = build_gold_set(authors, min_mentions=args.min_mentions)
    
    print("\nã€ORCIDé‡‘æ ‡å‡†ç»Ÿè®¡ / Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ³Ğ¾ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ°ã€‘")
    print(f"  æ€»mentions: {gold_set['stats']['total_mentions']}")
    print(f"  æœ‰ORCIDçš„mentions: {gold_set['stats']['mentions_with_orcid']}")
    print(f"  å”¯ä¸€ORCIDæ•°: {gold_set['stats']['unique_orcids']}")
    print(f"  è¿‡æ»¤åORCID (>={args.min_mentions} mentions): {gold_set['stats']['filtered_orcids']}")
    print(f"  é‡‘æ ‡å‡†mentionæ€»æ•°: {gold_set['stats']['mentions_in_gold_set']}")
    
    # 3. è¿è¡Œæ¶ˆæ­§
    logger.info("\nè¿è¡Œæ¶ˆæ­§ç®—æ³• / Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ°Ğ»Ğ³Ğ¾Ñ€Ğ¸Ñ‚Ğ¼Ğ° Ğ´Ğ¸Ğ·Ğ°Ğ¼Ğ±Ğ¸Ğ³ÑƒĞ°Ñ†Ğ¸Ğ¸...")
    config = {
        'mode': args.mode,
        'accept_threshold': args.accept_threshold,
        'reject_threshold': args.reject_threshold,
    }
    
    start_time = datetime.now()
    predicted_clusters, disamb_stats = run_disambiguation(authors, gold_set, config, logger)
    elapsed = (datetime.now() - start_time).total_seconds()
    
    print(f"\nã€æ¶ˆæ­§ç»Ÿè®¡ / Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ° Ğ´Ğ¸Ğ·Ğ°Ğ¼Ğ±Ğ¸Ğ³ÑƒĞ°Ñ†Ğ¸Ğ¸ã€‘")
    print(f"  å¤„ç†æ—¶é—´ / Ğ’Ñ€ĞµĞ¼Ñ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸: {elapsed:.2f}s")
    print(f"  æ€»å¤„ç†mentions: {disamb_stats['total_processed']}")
    print(f"  MERGEå†³ç­–: {disamb_stats['merge_decisions']}")
    print(f"  NEWå†³ç­–: {disamb_stats['new_decisions']}")
    print(f"  UNKNOWNå†³ç­–: {disamb_stats['unknown_decisions']}")
    print(f"  é¢„æµ‹clustersæ•°: {len(predicted_clusters)}")
    
    # 4. è¯„æµ‹
    logger.info("\nè¯„æµ‹æ¶ˆæ­§ç»“æœ / ĞÑ†ĞµĞ½ĞºĞ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ´Ğ¸Ğ·Ğ°Ğ¼Ğ±Ğ¸Ğ³ÑƒĞ°Ñ†Ğ¸Ğ¸...")
    
    bcubed = evaluate_bcubed(gold_set, disamb_stats['mention_to_predicted'])
    pairwise = evaluate_pairwise(gold_set, disamb_stats['mention_to_predicted'])
    
    print("\n" + "=" * 80)
    print("ã€è¯„æµ‹ç»“æœ / Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¾Ñ†ĞµĞ½ĞºĞ¸ / Evaluation Resultsã€‘")
    print("=" * 80)
    
    print(f"\nğŸ“Š BÂ³ F1 æŒ‡æ ‡ / ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ BÂ³ F1:")
    print(f"  Precision: {bcubed['precision']:.4f}")
    print(f"  Recall:    {bcubed['recall']:.4f}")
    print(f"  F1:        {bcubed['f1']:.4f}")
    
    print(f"\nğŸ“Š Pairwise æŒ‡æ ‡ / ĞœĞµÑ‚Ñ€Ğ¸ĞºĞ¸ pairwise:")
    print(f"  Precision: {pairwise['precision']:.4f}")
    print(f"  Recall:    {pairwise['recall']:.4f}")
    print(f"  F1:        {pairwise['f1']:.4f}")
    print(f"  TP: {pairwise['tp']}, FP: {pairwise['fp']}, FN: {pairwise['fn']}")
    print(f"  Gold pairs: {pairwise['gold_pairs']}, Predicted pairs: {pairwise['pred_pairs']}")
    
    print("=" * 80)
    
    # 5. ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    results = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'data_file': args.data_file,
            'limit': args.limit,
            'min_mentions': args.min_mentions,
            'mode': args.mode,
            'accept_threshold': args.accept_threshold,
            'reject_threshold': args.reject_threshold,
            'elapsed_seconds': elapsed,
        },
        'gold_set_stats': gold_set['stats'],
        'disambiguation_stats': {
            'total_processed': disamb_stats['total_processed'],
            'merge_decisions': disamb_stats['merge_decisions'],
            'new_decisions': disamb_stats['new_decisions'],
            'unknown_decisions': disamb_stats['unknown_decisions'],
            'predicted_clusters': len(predicted_clusters),
        },
        'evaluation': {
            'bcubed': bcubed,
            'pairwise': pairwise,
        }
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"\nç»“æœå·²ä¿å­˜ / Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹: {output_path}")
    
    print(f"\nâœ… è¯„æµ‹å®Œæˆ / ĞÑ†ĞµĞ½ĞºĞ° Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!")
    print(f"   ç»“æœæ–‡ä»¶ / Ğ¤Ğ°Ğ¹Ğ» Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²: {output_path}")


if __name__ == '__main__':
    main()
